{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_16_Deep Learning for text and  sequences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZ5JxPjriEakA3SdoouHAB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jibook/git-remote/blob/master/TensorFlow_16_Deep_Learning_for_text_and_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwe9SpXhrfK3",
        "outputId": "abc4f0c1-f272-43a8-e1e3-70ad718456d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  jena_climate_2009_2016.csv.zip\n",
            "  inflating: jena_climate_2009_2016.csv  \n",
            "  inflating: __MACOSX/._jena_climate_2009_2016.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip jena_climate_2009_2016.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "data_dir = './'\n",
        "fname = os.path.join(data_dir,'jena_climate_2009_2016.csv') "
      ],
      "metadata": {
        "id": "pmIDQfxfsiyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(fname)\n",
        "data = f.read()\n",
        "f.close() \n",
        "lines = data.split('\\n')\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:] "
      ],
      "metadata": {
        "id": "xLs2gGINsi1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines): \n",
        "  values = [float(x) for x in line.split(',')[1:]]\n",
        "  float_data[i, :] = values"
      ],
      "metadata": {
        "id": "90r0Pil8si3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = float_data[:200000].mean(axis=0)\n",
        "float_data -= mean\n",
        "std = float_data[:200000].std(axis=0)\n",
        "float_data /= std"
      ],
      "metadata": {
        "id": "uaTrRKtasi5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(data, lookback, delay, min_index, max_index,\n",
        "              shuffle=False, batch_size=128, step=6):\n",
        "  if max_index is None:\n",
        "    max_index = len(data) - delay - 1\n",
        "  i = min_index + lookback\n",
        "  while 1:\n",
        "    if shuffle:\n",
        "      rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
        "    else:\n",
        "      if i + batch_size >= max_index:\n",
        "        i = min_index + lookback\n",
        "      rows = np.arange(i, min(i + batch_size, max_index))\n",
        "      i += len(rows)\n",
        "    samples = np.zeros((len(rows),lookback // step,data.shape[-1]))\n",
        "    targets = np.zeros((len(rows),))\n",
        "    for j, row in enumerate(rows):\n",
        "      indices = range(rows[j] - lookback, rows[j], step)\n",
        "      samples[j] = data[indices]\n",
        "      targets[j] = data[rows[j] + delay][1]\n",
        "    yield samples, targets"
      ],
      "metadata": {
        "id": "ltmUxnNVsi8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookback = 1440\n",
        "step = 6\n",
        "delay = 144\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "2PgIALXTsi_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step,\n",
        "                      batch_size=batch_size)\n",
        "val_gen = generator(float_data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step,\n",
        "                     batch_size=batch_size)\n",
        "\n",
        "val_steps = (300000 - 200001 - lookback)\n",
        "test_steps = (len(float_data) - 300001 - lookback)"
      ],
      "metadata": {
        "id": "em_9YZzEsjCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers \n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "metadata": {
        "id": "mWXNSyvIsjE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential() \n",
        "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1)) \n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator( train_gen, \n",
        "                              steps_per_epoch=500, \n",
        "                              epochs=20, \n",
        "                              validation_data=val_gen, \n",
        "                              validation_steps=val_steps )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxU_tZ_3sjHd",
        "outputId": "060ab550-fcbe-4957-d387-742a34376490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500/500 [==============================] - 3379s 7s/step - loss: 0.3018 - val_loss: 0.2707\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 3336s 7s/step - loss: 0.2815 - val_loss: 0.2675\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 3388s 7s/step - loss: 0.2772 - val_loss: 0.2673\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 3388s 7s/step - loss: 0.2708 - val_loss: 0.2608\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 3369s 7s/step - loss: 0.2657 - val_loss: 0.2620\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 3377s 7s/step - loss: 0.2605 - val_loss: 0.2628\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 3438s 7s/step - loss: 0.2533 - val_loss: 0.2647\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.2508"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss'] \n",
        "epochs = range(1, len(loss) + 1) plt.figure() \n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss') \n",
        "plt.legend() \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BJDz3cR28rX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "너무 오래걸려,,,,"
      ],
      "metadata": {
        "id": "oKNXXbJhM-d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k9lhga7oNAUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}